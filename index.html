<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>CoBra</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>
 <script type="text/x-mathjax-config">
        MathJax.Hub.Config({            
            tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}            
        });
    </script>
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>

</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">CoBra: Complementary Branch Fusing Class and Semantic Knowledge for Robust Weakly Supervised Semantic Segmentation</h1>
          <div class="is-size-5 publication-authors">

            <span class="author-block">
              <a>Anonymous 9555</a>
            </span>
          </div>

  

          <div class="column has-text-centered">
            <div class="publication-links">
   
              <span class="link-block">
                <a href="https://github.com/anno9555"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>

            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="./static/images/web0.png" alt="main" height="100%">
      <h2 class="subtitle has-text-centered">
        Overview illustration of our model, <b>Co</b>mplementary <b>Bra</b>nch (<b>CoBra</b>).  <br>        
        The dual branch framework consists of the Class-Aware Knowledge branch with CNN and the Semantic-Aware Knowledge branch with ViT. They give complementary knowledge to each branch.
        </h2>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Motivation. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Motivation</h2>
        <div class="content has-text-justified">
          <img src="./static/images/motivation.png" alt="main" height="100%">
          While Class Activation Maps (CAMs) using CNNs have steadily been contributing to the success of WSSS, the resulting activation maps often narrowly focus on class-specific parts (e.g., only face of human). On the other hand, recent works based on vision transformers (ViT) have shown promising results based on their self-attention mechanism to capture the semantic parts but fail in capturing complete class-specific details (e.g., entire body parts of human but also with a dog nearby).  <br>
         The figure shows the comparison of object localization maps from each CNN, ViT, and Cobra branches for various subjects (human, dog, airplane), illustrating the distinctive areas of interest each model identifies. Our model successfully utilizes <b>complementary characteristics to localize the exact object of the correct class and its semantic parts.</b>
          
        </div>
      </div>
    </div>
  </div>
</section>

  
<section class="section">
  <div class="container is-max-desktop">
    <!-- Key Contribution. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Key Contribution</h2>
        <div class="content has-text-justified">
          <li>We propose a dual branch framework, namely <b>Co</b>mplementary <b>Bra</b>nch (<b>CoBra</b>), which aims to fuse the complementary nature of CNN and ViT localization maps. </li>
          <li>We capture the class and semantic knowledge as Class Aware Projection (CAP) and Semantic-Aware Projection (SAP) respectively for effective complementary guidance to the CNN and ViT branches in CoBra, employing contrastive learning for enhanced guidance. </li>
          <li>Extensive experiments qualitatively and quantitatively investigate how CNN and ViT complement each other on the PASCAL VOC 2012 dataset and MS COCO 2014 dataset, showing a state-of-the-art WSSS result. </li>
          </div>
      </div>
    </div>
  </div>
</section>

  <section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h3 class="title is-3 has-text-centered">Main model</h3>
          <div class="content has-text-centered">
            <img src="./static/images/main.png" alt="Main figure">
          </div>
          <div class="content has-text-justified">
            <p>
              Overview illustration of our model. <br>
      <b>(I) Class Aware Knoweldge(CAK)</b>: The CNN outputs a feature map which generates (1) CNN CAMs via $f_{CAM}$, (2) Pseudo-Labels from CNN CAMs via $argmax$, and (3) Class-Aware Projection (CAP) via $f_{proj}$. <br>
              
      <b>(II) Semantic Aware Knowledge(SAK)</b>: The ViT outputs $N^2$ Patch Embeddings which generate (1) ViT CAMs via $f_{CAM}$ and (2) Semantic-Aware Projection (SAP) via $f_{proj}$. We also use the Attention Maps of all $L$-layers to generate (3) Patch Affinity of size $N^2 \times N^2$.
            
            </p>
          </div>
      </div>
    </div>
    
  </div>
</section>
  
  
<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h3 class="title is-3 has-text-centered">Method</h3>
          <div class="content has-text-centered">
            <img src="./static/images/main2.png" alt="Main figure">
          </div>
          <div class="content has-text-justified">
            <p>
              Illustration of refining CAP and SAP from SAK and CAK branch respectively. <br>
      <b>(I) Class Aware Knoweldge(CAK)</b>: The CAP values are embedded in the Class Feature Space. (1) The patch affinity from SAK branch assigns the positive (green), negative (red), and neutral (teal) patches based on the target (white) patch. (2) The CNN CAM shows that the false negative patches have been weakly localized as horse. (3) The CAP loss pull those weakly localized patches (i.e., false class negatives) since they are assigned as semantically positive patches based on SAK branch. (3) The CAP is refined to improve the CNN CAM showing fewer false class negatives. <br>
      <b>(II) Semantic Aware Knowledge(SAK)</b>: The SAP values are embedded in the Semantic Feature Space. (1) The CNN CAM from CAK branch assigns the positive (green), negative (red), and neutral (teal) patches based on the target (white) patch. (2) The ViT CAM shows that the negative patches have been incorrectly localized as horse. The SAP loss pushes away those incorrectly localized patches (i.e., false class positives) since they are assigned as negative patches based on CAK branch. (3) The SAP is refined to improve the ViT CAM showing fewer false class positives.
            </p>
          </div>
      </div>
    </div>
    
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h3 class="title is-3 has-text-centered">Quantitative Experiments</h3>
        
        <h4 class="title is-3 has-text-centered">Pascal VOC 2012 seed & mask results</h3>
        <div class="content has-text-centered">
          <img src="./static/images/table1.png" alt="Main figure">
        </div>
        <p>
       Evaluation of initial seed and corresponding pseudo segmentation mask on PASCAL VOC 2012 training set in mIoU (%).
        </p>
        <br>
        <br>
        <h4 class="title is-3 has-text-centered">Pascal VOC 2012 segmentation results</h3>
        <div class="content has-text-centered">
          <img src="./static/images/table2.png" alt="Main figure">
        </div>
        <p>
        Semantic segmentation results on the validation (Val) and Test set of PASCAL VOC 2012 dataset. Sup. (Supervision) : Image (I) and Saliency Map (S).
        </p>
        <br>
        <br>
        <h4 class="title is-3 has-text-centered">MS-COCO 2014 segmentation results</h3>
        <div class="content has-text-centered">
          <img src="./static/images/Table3.png" alt="Main figure">
        </div>
        <div class="content has-text-justified">
        <p>
        Semgentation mIoU results(%) on MS-COCO 2014 val dataset
          <br>
          <br>
        </p>
      </div>
      </div>
    </div>
    
  </div>
</section>
  
<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">
      <div class="column is-four-fifths">
        
        <h3 class="title is-3 has-text-centered">Qualitative Experiments</h3>
        
        <h4 class="title is-3 has-text-centered">Seed Results</h4>
        
        <div class="content has-text-centered">
          <img src="./static/images/main3.png" alt="Main figure">
        </div>
        <div class="content has-text-justified">
          <p>
            Qualitative results. From left: (1) Input image, (2) Our result, (3) CNN CAM of our model, (4) Ours without SAP Loss, (5) ViT CAM of our model, (6) Ours without CAP Loss, (7) Our Pseudo mask for segmentation and (8) ground-truth segmentation label. We see that our results are able to differentiate between classes while finding their accurate object boundaries.
          </p>
        </div>
      </div>
    </div>
    
  </div>
</section>
  
<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h3 class="title is-3 has-text-centered">Pascal VOC Segmentation Results</h3>
        
        <div class="content has-text-centered">
          <img src="./static/images/pascal.png" alt="Main figure">
        </div>
        <div class="content has-text-justified">
          <p>
            Qualitative seg results on the PASCAL VOC val set.
          </p>
        </div>
      </div>
    </div>
    
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h3 class="title is-3 has-text-centered">MS COCO Segmentation Results</h3>
        
        <div class="content has-text-centered">
          <img src="./static/images/coco.png" alt="Main figure">
        </div>
        <div class="content has-text-justified">
          <p>
            Qualitative seg results on the MS COCO val set.
          </p>
        </div>
      </div>
    </div>
    
  </div>
</section>
  

</section>


<footer class="footer">
  <div class="container">

    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
