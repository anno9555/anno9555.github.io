<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>CoBra</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>

</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">CoBra: Complementary Branch Fusing Class and Semantic Knowledge for Robust Weakly Supervised Semantic Segmentation</h1>
          <div class="is-size-5 publication-authors">

            <span class="author-block">
              <a>Anonymous 9555</a>
            </span>
          </div>

  

          <div class="column has-text-centered">
            <div class="publication-links">
   
              <span class="link-block">
                <a href="https://github.com/anno9555"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>

            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="./static/images/main1.png" alt="main" height="100%">
      <h2 class="subtitle has-text-centered">
      Overview illustration of our model, Cross Complementary Branch (**CoBra**). The dual branch framework consists of the Class-Aware Knowledge branch with CNN (top) and the Semantic-Aware Knowledge branch with ViT (bottom). The input image is passed down to both branches. **Class-Aware Knowledge (CAK) Branch**: The CNN outputs a feature map which generates (1) CNN CAMs via $f_{CAM}$, (2) Pseudo-Labels from CNN CAMs via $argmax$, and (3) Class-Aware Projection (CAP) via $f_{proj}$.
    \textbf{Semantic-Aware Knowledge (SAK) Branch}: The ViT outputs $N^2$ Patch Embeddings which generate (1) ViT CAMs via $f_{CAM}$ and (2) Semantic-Aware Projection (SAP) via $f_{proj}$. We also use the Attention Maps of all $L$-layers to generate (3) Patch Affinity of size $N^2 \times N^2$.
    \textbf{Complementary Branch Losses}: Once the necessary outputs are prepared, we employ various losses: (1) $\mathcal{L}_{cls}$: The typical classification loss based on the individual classification predictions of each CNN CAM and ViT CAM. (2) $\mathcal{L}_{cam}$: The L1 loss between the CNN CAM and ViT CAM. (3) $\mathcal{L}_{sap}$: The class-aware knowledge from the pseudo labels guides SAP to identify more accurate class-specific patches. (4) $\mathcal{L}_{cap}$: The semantic-aware knowledge from the patch affinity improves the semantic sensitivity of ViT CAM.
      </h2>
    </div>
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          Semantic segmentation has heavily depended on extensive labeled data, leading to the emergence of unsupervised methodologies.
          Among them, leveraging self-supervised Vision Transformers (ViT) for unsupervised semantic segmentation stands out for their noteworthy outcomes. 
          Yet, a predominant challenge remains: many existing methods primarily focus on patch-level relationships which result in the inability to accurately identify objects within an image or generate a significant amount of noise in the segmentation map.
          To tackle this issue, we present a novel approach, <i>EAGLE</i>, which emphasizes an object-centric perspective, essential for effective semantic segmentation. 
          Specifically, we employ the Laplacian matrix derived from the projected image feature along with the color affinity matrix from the image.
          By clustering its eigenvectors, our model captures the intrinsic non-linear structures of images, underscoring the object-centric perspective.
          Then, we integrate an object-level contrastive loss, ensuring that representations within an object are cohesively gathered.
          Furthermore, by leveraging hierarchical attention keys in ViT, our model identifies objects of varying scales and deepens its understanding of their relationships between objects.
          Through a series of well-constructed experiments on COCO-Stuff, Cityscapes, and Potsdam-3 datasets, EAGLE showcases its state-of-the-art capabilities.
        </div>
      </div>
    </div>

        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Video</h2>
            <div class="publication-video">
              <iframe src="https://www.youtube.com/embed/jNQXAC9IVRw?si=K-u3llTqxptv-xLm"
                      frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
            </div>
          </div>
        </div>

      </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h3 class="title is-3 has-text-centered">Method</h3>
        
        <div class="content has-text-centered">
          <img src="./static/images/main4.png" alt="Main figure">
        </div>
        <div class="content has-text-justified">
          <p>
            The pipeline of EAGLE. 
            Leveraging the Laplacian matrix, which integrates hierarchically projected image key features and color affinity, the model harnesses eigenvector clustering to capture an object-centric perspective.
             Our model further adopts an object-level contrastive loss, utilizing the projected vector Z and  ̃Z. 
             The learnable prototype Φ, acts as a singular anchor that contrasts positive objects and negative objects. 
             Our object-level contrastive loss is computed in two distinct manners: crosswise and non-crosswise to ensure semantic consistency.
          </p>
        </div>
      </div>
    </div>
    
  </div>
</section>

<footer class="footer">
  <div class="container">

    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
